# UnifiedLog
Transformer-Based Log Anomaly Detection with Cross-Dataset Generalization


## Abstract
A novel method with a strong generalization ability for detecting anomalies in log data is presented, which enables the detection of anomalies even in log types that were not seen during training. To achieve this, a transformer-based encoder is trained in a self-supervised manner on a large volume of multiple log data sets simultaneously. Using the representation generated by the encoder as a fundamental model, the detector part of the method is trained in a supervised manner on one or more data sets, where training with more data sets leads to better results. Furthermore, when trained with multiple data sets, the method can also generalize to a data set that was not included in the training.
The study employs two encoder models, which differ in the tokenization of digits. Together, these models can achieve results that are at least as good as, or better than, those of the state-of-the-art solutions on the most important log data benchmarks.
Our results are supported by numerous measurements, for which we make the codes available.
<br>
<br>

## Usage 
Creating the conda environment:<br>
<code>conda env create -f environment.yml</code>
<br>
<br>
Download the datasets available on Loghub with <i>loghub_downloader.py</i>.
<br>
<code>python3 loghub _downloa.py -s \<save_folder\></code>
<br>
<br>
Preprocess datasets with <i>data_preprocess.py</i>.
<br>
<code>python3 data_preproceess.py -d \<path_to_downloaded_logs\> -s \<save_folder\> -l <cap_maximum_lines_per_dataset/> -t \<num_of_tokens(default=1002)\></code>
This script trains a Wordpiece tokenizer on all the datasets and tokenizes them. Three folders are created by this scipt: <tokenized\>, <tokenized_for_detector/> and <labels/>.
<br>
<br>
To train UnifiedLog run the <i>run.py</i> script.
<br>
<code>python3 run.py -c \<conf_file\> -t \<cpu_threads\></code>
<br>
Example config file:
<code>
name: all_model_16_benchmark_0_9_special_token_vocab_size_1002 # Name of the run in neptune
neptune_logging: true
transformer_encoder:
  train_paths: "path_to/tokenized/" # Folder containing data tokenized for the encoder (also accepts a list directly containing files)
  load_path: "path_to/saved_model" # Load encoder from previous save
  save_path: "path_to/model_name" # Save path of the encoder
  save_every_epoch: True # If True a model with save_path + _epoch_n.pkl will be saved every epoch
  train_val_test_split: [0.8, 0.9]
  mask_prob: 0.15
  replace_prob: 0.9
  num_tokens: 1004
  max_seq_len: 128
  attn_layers:
    dim: 16 # This affects the detector part's embedding also
    depth: 4
    heads: 6
  batch_size: 4096
  lr: 0.00003
  epochs: 5
  mask_token_id: 1002
  pad_token_id: 1003
  max_train_data_size: 10000000      # Cap maximum lines used from one dataset for training
anomaly_detector:
  train_paths: "path_to/tokenized_for_detector" # folder created by data_preprocess.py
  label_paths: "path_to/labels" # folder created by data_preprocess.py
  test_data_paths: "path_to/tokenized_for_detector"
  test_labels: "path_to/labels" 
  load_path: null # Load detector from previous save
  save_path: null # Save path of the detector
  train_val_test_split: [0.8, 0.9] 
  lr_decay_step_size: 25
  lr_decay_gamma: 0.9
  early_stop_tolerance: 3
  early_stop_min_delta: 0
  batch_size: 64
  epochs: 200
  embed_dim: 64
  ff_dim: 256
  max_len: 20
  num_heads: 8
  dropout: 0.5
  lr: 0.00003
  balancing_ratio: 1
</code>
